{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lim-jr/ESAA/blob/main/0407%EA%B8%88%EC%84%B8%EC%85%98_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "1348021b-18ae-4fd5-ed6b-ed5b9a1dbd5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "GowligokZeEA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(word_tokens)\n",
        "\n",
        "word_tokens1 = word_tokenize(text)\n",
        "print(word_tokens1)"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f8bc12-99c5-4cc5-df12-f49bc4facadd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.'], ['I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.'], ['My', 'favourite', 'color', 'is', 'black']]\n",
            "['a', 'new', 'entry', 'in', 'the', '``', 'revisionist', 'history', '``', 'genre', 'of', 'filmmaking', ',', 'dick', 'suggests', 'that', 'two', 'not-too-bright', 'teenage', 'girls', 'are', 'the', 'cause', 'of', 'the', 'uncovering', 'of', 'the', 'nation', \"'s\", 'biggest', 'presidential', 'scandal', '.', 'kirsten', 'dunst', 'and', 'michelle', 'williams', 'star', 'betsy', 'and', 'arlene', ',', 'who', 'while', 'trying', 'to', 'deliver', 'a', 'fan', 'letter', 'from', 'arlene', \"'s\", 'watergate', 'hotel', 'room', ',', 'accidentally', 'stumble', 'across', 'g', '.', 'gordon', 'liddy', '(', 'played', 'dead-on', 'by', 'harry', 'shearer', ')', 'and', 'the', 'infamous', 'break-in', '.', 'when', 'they', 'recognize', 'liddy', 'later', 'on', 'during', 'a', 'white', 'house', 'field', 'trip', ',', 'they', 'are', 'ushered', 'into', 'a', 'conference', 'room', ',', 'questioned', 'as', 'to', 'what', 'they', 'know', ',', 'and', 'leave', 'as', 'official', 'presidential', 'dog', 'walkers', '.', 'the', 'girls', 'manage', 'to', 'unwittingly', 'uncover', 'every', 'bit', 'of', 'the', 'watergate', 'scandal', 'while', 'performing', 'their', 'duties', ',', 'but', 'have', 'no', 'clue', 'as', 'to', 'what', 'they', 'are', 'getting', 'involved', 'with', '.', 'when', 'they', 'discover', 'that', 'nixon', '(', 'another', 'dead-on', 'performance', 'by', 'dan', 'hedaya', ',', 'who', 'actually', 'favors', 'nixon', 'slightly', ',', 'unlike', 'anthony', 'hopkins', ')', 'has', 'been', 'abusive', 'to', 'checkers', ',', 'the', 'presidential', 'dog', ',', 'thanks', 'to', 'the', 'conversations', 'that', 'he', 'always', 'recorded', ',', 'they', 'quit', 'and', 'become', 'disillusioned', '.', 'during', 'a', 'prank', 'phone', 'call', 'the', 'girls', 'make', 'to', 'woodward', 'and', 'bernstein', ',', 'events', 'are', 'set', 'into', 'motion', 'that', 'eventually', 'lead', 'to', 'the', 'president', \"'s\", 'resignation', '.', 'this', 'film', 'starts', 'off', 'promisingly', 'with', 'an', 'aged', 'woodward', 'and', 'bernstein', 'arguing', 'with', 'each', 'other', 'on', 'an', 'obvious', 'larry', 'king-type', 'talk', 'show', '(', 'featuring', 'a', 'cameo', 'by', 'french', 'stewart', ')', 'about', 'revealing', 'the', 'identity', 'of', '``', 'deep', 'throat', '``', '.', 'from', 'there', ',', 'we', 'are', 'subjected', 'to', 'bodily', 'function', 'humor', 'and', 'just', 'about', 'every', 'bad', '``', 'dick', '``', 'joke', 'one', 'can', 'derive', 'from', 'this', 'type', 'of', 'supposed', 'comedy', '.', 'at', 'one', 'point', ',', 'the', 'girls', 'are', 'having', 'to', 'scream', 'over', 'a', 'high', 'school', 'band', 'playing', 'on', 'the', 'steps', 'of', 'the', 'lincoln', 'memorial', '.', 'the', 'band', 'manages', 'to', 'stop', 'right', 'as', 'dunst', 'screams', '``', 'you', 'have', 'to', 'stop', 'letting', 'dick', 'run', 'your', 'life', '!', '``', 'much', 'to', 'the', 'horror', 'of', 'everyone', 'standing', 'within', 'earshot', '.', 'several', 'other', 'variations', 'on', 'this', 'wordplay', 'surface', 'all', 'throughout', 'the', 'film', '.', 'if', 'this', 'movie', 'had', 'been', 'smarter', 'i', 'would', 'have', 'been', 'less', 'likely', 'to', 'fault', 'it', \"'s\", 'juvenile', 'bathroom', 'humor', ',', 'but', 'it', \"'s\", 'not', '.', 'the', 'film', 'was', 'apparently', 'made', 'for', 'relatively', 'younger', 'people', 'because', 'every', 'major', 'player', 'in', 'the', 'watergate', 'scandal', 'is', 'introduced', 'and', 'shoved', 'down', 'the', 'audience', \"'s\", 'throat', 'in', 'the', 'least', 'subtle', 'way', 'possible', '.', 'i', 'do', \"n't\", 'recall', 'oliver', 'stone', \"'s\", 'nixon', 'having', 'to', 'pander', 'to', 'it', \"'s\", 'audience', ',', 'but', 'of', 'course', 'that', 'film', 'was', \"n't\", 'a', 'comedy', 'aimed', 'squarely', 'at', 'a', '13-20', 'year-old', 'film', 'going', 'audience', '.', 'the', 'only', 'redeeming', 'thing', 'about', 'this', 'movie', 'is', 'it', \"'s\", 'remarkable', 'supporting', 'cast', '.', 'i', 'wanted', 'to', 'see', 'more', 'of', 'ferrell', 'and', 'mcculloch', \"'s\", 'woodward', 'and', 'bernstein', '.', 'those', 'two', 'characters', 'are', 'the', 'sole', 'basis', 'for', 'my', 'rating', '.', 'i', 'wish', 'they', 'had', 'been', 'given', 'more', 'screen', 'time', ',', 'but', 'unfortunately', ',', 'they', 'are', 'only', 'relegated', 'to', 'the', 'final', 'half-hour', '.', 'their', 'constant', 'bickering', 'and', 'fighting', 'over', 'trying', 'to', 'get', 'the', 'story', 'are', 'a', 'major', 'highlight', ',', 'especially', 'mcculloch', \"'s\", 'constant', 'thwarting', 'of', 'ferrell', \"'s\", 'attempts', 'to', 'gather', 'information', 'from', 'the', 'girls', '(', 'who', ',', 'in', 'the', 'course', 'of', 'the', 'narrative', 'are', 'revealed', 'as', 'deep', 'throat', ',', 'so', 'named', 'thanks', 'to', 'an', 'ill', 'planned', 'trip', 'to', 'a', 'porno', 'theater', 'by', 'betsy', \"'s\", 'brother', ')', '.', 'the', 'other', 'members', 'of', 'the', 'cast', 'are', 'excellent', 'in', 'their', 'portrayals', 'of', 'their', 'particular', 'characters', ',', 'but', 'are', 'given', 'nothing', 'to', 'work', 'with', '.', 'i', \"'d\", 'like', 'to', 'see', 'the', 'same', 'cast', 'portray', 'these', 'characters', 'in', 'a', 'script', 'more', 'suited', 'towards', 'their', 'comedic', 'abilities', '.', 'as', 'for', 'the', 'two', 'leads', ',', 'dunst', 'and', 'williams', 'can', 'definitely', 'do', 'better', '.', 'they', 'come', 'off', 'as', 'what', 'could', 'best', 'be', 'described', 'as', 'romy', 'and', 'michele', ':', 'the', 'early', 'years', 'in', 'this', 'particular', 'film', ',', 'a', 'highly', 'dubious', 'distinction', 'at', 'best', '.', 'stay', 'through', 'the', 'first', 'half', 'of', 'the', 'end', 'credits', 'though', ',', 'to', 'see', 'an', 'interesting', 'scene', 'involving', 'dunst', 'and', 'williams', 'suggestively', 'sucking', 'on', 'lollipops', 'emblazoned', 'with', 'the', 'title', 'of', 'the', 'movie', '.', 'an', 'excellent', 'idea', 'marred', 'by', 'poor', 'execution', ',', 'dick', 'could', 'have', 'been', 'a', 'great', 'movie', '.', 'less', 'of', 'the', 'juvenile', 'humor', 'and', 'more', 'of', 'the', 'smarter', 'comedy', 'displayed', 'by', 'the', 'woodward', 'and', 'bernstein', 'scenes', ',', 'could', 'have', 'made', 'this', 'film', 'a', 'wonderful', 'satire', 'of', 'the', 'nixon', 'presidency', 'as', 'seen', 'through', 'the', 'eyes', 'of', 'two', 'naive', 'fifteen', 'year', 'olds', '.', 'as', 'it', 'stands', 'though', ',', 'dick', 'offers', 'nothing', 'but', 'what', 'filmmaker', 'kevin', 'smith', 'so', 'accurately', 'defines', 'as', '``', 'dick', 'and', 'poopie', '``', 'jokes', '.', 'and', 'that', ',', 'to', 'me', ',', 'does', 'not', 'make', 'a', 'funny', 'movie', '.', '[', 'pg-13', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords  "
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d31c590-da82-41bf-bf70-8e7184aa4f34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.corpus.stopwords.words('english')[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agUteif2Gc1t",
        "outputId": "cee21516-7b8f-4fbe-bd62-5cf771c0b763"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "\n",
        "all_tokens = []\n",
        "\n",
        "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "  filtered_words=[]\n",
        "  # 개별 문장별로 토큰화된 문장 list에 대해 스톱워드를 제거하는 반복문\n",
        "  for word in sentence:\n",
        "    # 소문자로 모두 변환\n",
        "    word=word.lower()\n",
        "    # 토큰화된 개별단어가 스톱워드에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens) \n",
        "print(word_tokens)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7a47c3-903f-4142-db48-772d0dea3b25"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hi', ',', 'name', 'amartya', 'nambiar', '.'], ['computer', 'science', 'engineer', '.'], ['favourite', 'color', 'black']]\n",
            "[['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.'], ['I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.'], ['My', 'favourite', 'color', 'is', 'black']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = [word_tokens if w]"
      ],
      "metadata": {
        "id": "bRfKNhKvL0fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "\n",
        "for i in range(len(all_tokens)):\n",
        "  if '.' in all_tokens[i]:\n",
        "    all_tokens[i].remove('.')\n",
        "  if ',' in all_tokens[i]:\n",
        "    all_tokens[i].remove(',')\n",
        "\n",
        "all_tokens"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22685f9f-d543-4368-99c3-faf8eb34a235"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hi', 'name', 'amartya', 'nambiar'],\n",
              " ['computer', 'science', 'engineer'],\n",
              " ['favourite', 'color', 'black']]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "# example1= ['helps', 'helping', 'helped']\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('helps'), stemmer.stem('helping'), stemmer.stem('helped'))"
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aef042c-935f-4633-dd0b-ab6d94d2f797"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "help help help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ps.stem('happiness') # but it isn't always the best choice\n",
        "\n",
        "print(stemmer.stem('happiness'))"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9f5850-6c39-4eca-cbf9-0d8109c797b5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective."
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\") "
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58b6996-913f-4623-f300-e1b7e8bd8aad"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('new', 'JJ'), ('entry', 'NN'), ('in', 'IN'), ('the', 'DT')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply pos_tag(), print result\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = pos_tag(word_tokenize(text))     #applying pos_tag()\n",
        "pos[1:5] "
      ],
      "metadata": {
        "id": "vcWgmScAedLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199f94b4-6bf2-4dc0-ac7d-545a7632675f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('new', 'JJ'), ('entry', 'NN'), ('in', 'IN'), ('the', 'DT')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a09620-262f-497f-c232-b6c6647a2039"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "print(lemma.lemmatize('believes'), lemma.lemmatize('believes', 'a'), lemma.lemmatize('believes', 'v'))\n",
        "\n",
        "print(lemma.lemmatize('happiness'), lemma.lemmatize('happiness', 'a'), lemma.lemmatize('happiness', 'v'))"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a657be53-bbba-44de-c0ec-04ad9387e1f2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "belief believes believe\n",
            "happiness happiness happiness\n"
          ]
        }
      ]
    }
  ]
}